# mysql基础


# 大表优化

当对一个数据量大的表进行crud操作时，性能会明显下降。

解决方法有：

* 限定数据的范围：禁止不带任何数据范围条件的操作。比如，对用户订单查询，应该限制在一个月或者一周。

* 读写分离：主库负责写，从库负责读。

* 垂直分区：把一个具有20列的表拆分为两个10列的表，但要保证每个表都有primary key。但垂直分区主键出现了冗余，有可能需要使用join操作。

* 水平分区：保证数据row不变，把数据库的行拆分。比如把一个100万条数据的数据库拆分为两个50万条数据的数据库。  

# 池化思想

数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是
维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程
序的请求，既昂贵又浪费资源。在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个
新连接并将其添加到池中。 连接池还减少了用户必须等待建立与数据库的连接的时间。

# 分块分区后的主键处理

* UUID：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。

* 数据库自增 id : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。

* 利用 redis 生成 id : 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。

* Twitter的snowflake算法 ：Github 地址：https://github.com/twitter-archive/snowflake。

* 美团的Leaf分布式ID生成系统 ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。美团技术团队的一篇文章：https://tech.meituan.com/2017/04/21/mt-leaf.html 。


# 数据库的范式

https://www.cnblogs.com/rosesmall/p/9585655.html

**第一范式**：当**关系模式R的所有属性都不能再分解为更基本的数据单位时，称R是满足第一范式的**，简记为1NF。满足第一范式是关系模式规范化的最低要求，否则，将有很多基本操作在这样的关系模式中实现不了。

1、每一列属性都是不可再分的属性值，确保每一列的原子性

2、两列的属性相近或相似或一样，尽量合并属性一样的列，确保不产生冗余数据

------

如果关系模式R是第一范式的模式，那么，R的每一个关系r的属性都是**原子项**，不可分割。

1NF是关系模式应具备的最起码的条件，如果数据库设计不能满足第一范式，就不能称为关系型数据库。关系数据库设计研究的关系规范化是在1NF之上进行的。

------

**第二范式**：如果关系模式R满足第一范式，并且R的**所有非主属性都完全依赖于R的候选关键属性**，称R满足第二范式，简记为2NF。

如果关系模式R是1NF，且每一个非主属性完全依赖于候选建，那么就称R是第二范式。

第二范式要满足的条件：首先要满足第一范式，其次每一个非主属性要完全函数依赖于候选键，或者是主键。也就是说，每个非主属性是由整个主键函数决定的，而不能有主键的一部分来决定。

每一行的数据只能与其中一列相关，即一行数据只做一件事。只要数据列中出现数据重复，就要把表拆分开来。

比如：一个人同时订几个房间，就会出来一个订单号多条数据，这样子联系人都是重复的，就会造成数据冗余。我们应该把他拆开来。

**再举例一个不满足第二范式的设计：**

![1571725208847](D:\-\学习笔记md\图片\1571725208847.png)

**第三范式**：设R是一个满足第一范式条件的关系模式，X是R的任意属性集，如果X非传递依赖于R的任意一个候选关键字，称R满足第三范式，简记为3NF.

**如果关系模式R是2NF，且关系模式R（U,F）中的所有非主属性对任何候选关键字都不存在传递依赖，则称关系R是属于第三范式。**

**第三范式（3NF）；符合2NF，并且，消除传递依赖。**

**数据不能存在传递关系**，即没个属性都跟主键有直接关系而不是间接关系。像：a-->b-->c  属性之间含有这样的关系，是不符合第三范式的。

比如Student表（学号，姓名，年龄，性别，所在院校，院校地址，院校电话）

这样一个表结构，就存在上述关系。 学号--> 所在院校 --> (院校地址，院校电话)

这样的表结构，我们应该拆开来，如下。

（学号，姓名，年龄，性别，所在院校）--（所在院校，院校地址，院校电话）

## 其他范式

第四范式：要求把同一表内的多对多关系删除。

第五范式：从最终结构重新建立原始结构。

BC范式（BCNF）：符合3NF，并且，主属性不依赖于主属性。若关系模式R属于第一范式，且每个属性都不传递依赖于键码，则R属于BC范式。

# 数据库连接池

连接池用于创建和管理数据库连接的缓冲池技术，缓冲池中的连接可以被任何需要他们的线程使用。当一个线程需要用JDBC对一个数据库操作时，将从池中请求一个连接。当这个连接使用完毕后，将返回到连接池中，等待为其他的线程服务。 连接池的主要优点有以下三个方面。

第一、**减少连接创建时**间。连接池中的连接是已准备好的、可重复使用的，获取后可以直接访问数据库，因此减少了连接创建的次数和时间。

第二、**简化的编程模式**。当使用连接池时，每一个单独的线程能够像创建一个自己的JDBC连接一样操作，允许用户直接使用JDBC编程技术。

第三、**控制资源的使用**。如果不使用连接池，每次访问数据库都需要创建一个连接，这样系统的稳定性受系统连接需求影响很大，很容易产生资源浪费和高负载异常。连接池能够使性能最大化，将资源利用控制在一定的水平之下。连接池能控制池中的连接数量，增强了系统在大量用户应用时的稳定性。

### Q：如果连接池中的连接都被使用完了呢？

如果没有空闲连接，则查看当前所开的连接数是否已经达到最大连接数，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。 当客户释放数据库连接时，**先判断该连接的引用次数是否超过了规定值，如果超过就从连接池中删除该连接，否则保留为其他客户服务。**

————————————————————————————————————————————

连接池的工作原理主要由三部分组成，分别为**连接池的建立、连接池中连接的使用管理、连接池的关闭**。

第一、连接池的建立。一般在系统初始化时，连接池会根据系统配置建立，并在池中创建了几个连接对象，以便使用时能从连接池中获取。连接池中的连接不能随意创建和关闭，这样避免了连接随意建立和关闭造成的系统开销。Java中提供了很多容器类可以方便的构建连接池，例如Vector、Stack等。

第二、连接池的管理。连接池管理策略是连接池机制的核心，连接池内连接的分配和释放对系统的性能有很大的影响。其管理策略是：

当客户请求数据库连接时，首先查看连接池中是否有空闲连接，如果存在空闲连接，则将连接分配给客户使用；如果没有空闲连接，则查看当前所开的连接数**是否已经达到最大连接数**，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。 当客户释放数据库连接时，先判断该连接的引用次数是否超过了规定值，如果超过就从连接池中删除该连接，否则保留为其他客户服务。

该策略保证了数据库连接的有效复用，避免频繁的建立、释放连接所带来的系统资源开销。

第三、连接池的关闭。当应用程序退出时，关闭连接池中所有的连接，释放连接池相关的资源，该过程正好与创建相反。

### mysql性能调优（包含sql优化）

1.查看慢查询日志，定位执行效率差的SQL

2.需要知道该sql的执行，使用explain。

EXPLAIN命令是查看优化器如何决定执行查询的主要方法。

#### 临时表优化：

（临时表分内存临时表和磁盘临时表，如果是巨大的临时表，内存装不下，就会拷贝到磁盘）

如果表的设计已经确定，修改比较困难，那么也可以通过优化SQL语句来减少临时表的大小，以提升SQL执行效率。
常见的**优化SQL语句**方法如下：
1）拆分SQL语句
临时表主要是用于排序和分组，很多业务都是要求排序后再取出详细的分页数据，**这种情况下可以将排序和取出详细数据拆分成不同的SQL，以降低排序或分组时临时表的大小**，提升排序和分组的效率，我们的案例就是采用这种方法。
2）优化业务，去掉排序分组等操作
有时候业务其实并不需要排序或分组，仅仅是为了好看或者阅读方便而进行了排序，例如数据导出、数据查询等操作，这种情况下去掉排序和分组对业务也没有多大影响。

**设计优化**

使用临时表一般都意味着性能比较低，特别是使用磁盘临时表，性能更慢，因此我们**在实际应用中应该尽量避免临时表的使用**。 常见的避免临时表的方法有：
1）创建索引：**在ORDER BY或者GROUP BY的列上创建索引**；
2）**分拆很长的列**：一般情况下，TEXT、BLOB，大于512字节的字符串，基本上都是为了显示信息，而不会用于查询条件， 因此表设计的时候，应该将这些列独立到另外一张表。

- **如何判断使用了临时表？**

使用explain查看执行计划，Extra列看到Using temporary就意味着使用了临时表。

### 分表

目的：减少数据查询所需要的时间，提高数据库的吞吐量。

只能够对数据库的读进行扩展，而对数据库的写入操作还是集中在Master上，并且单个Master挂载的Slave也不可能无限制多，Slave的数量受到Master能力和负载的限制。

**对于访问极为频繁且数据量巨大的单表来说，我们首先要做的就是减少单表的记录条数，以便减少数据查询所需要的时间，提高数据库的吞吐，这就是所谓的分表**

对于互联网企业来说，大部分数据都是与用户关联的，因此，用户id是最常用的分表字段。因为大部分查询都需要带上用户id，这样既不影响查询，又能够使数据较为均衡地分布到各个表中

### 分库

目的：提高数据库写入能力

分表能够解决单表数据量过大带来的查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。面对高并发的读写访问，当数据库master服务器无法承载写操作压力时，不管如何扩展slave服务器，此时都没有意义了。

Q：分表和master-slave有什么关系？

一张表和两张表有什么区别？查询速度变快，然后呢？和这个主从架构有什么关系吗，这个架构带来的好处是可以分散读压力，可是不分表，读压力就？分表并不会增加数据库实例不是吗？



### 分库分表带来的坏处

ACID解决方法

**分布式事务问题**：使用两阶段提交协议

**水平切分递增ID被破坏**：多机sequence问题

解决方法：从连续性和唯一性考虑，考虑唯一性的话，可以使用uuid，根据自己的业务情况使用各种种子（不同纬度的表示，例如IP，MAC，机器名，时间，本机计数器等因素）来生成唯一的ID。这样生成的ID虽然保证了唯一性，但是在整个分布式系统中的连续性不好。

如果需要满足连续性（这里的连续性是指在整个分布式环境中生成的ID的连续性），我们可以用一个独立的系统来完成这个工作：吧所有的ID集中在一个地方进行管理，对每个ID序列独立管理，每台机器使用ID时都从这个ID生成器上进行获取。

跨库join：

可以进行部分数据冗余

（数据冗余如何同步<https://www.w3cschool.cn/architectroad/architectroad-redundant-table.html>）

# 索引

## 为什么要使用索引？

- 通过创建**唯一性索引**，可以保证数据库表中每一行数据的**唯一性**
- 可以大大加快数据的检索速度（大大减少检索的数据量），这也是创建索引的**最主要**原因。
- 帮助服务器避免排序和临时表
- 将随机IO变为顺序IO
- 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。

## 索引这么多优点，为什么不对表中每一列创建一个索引呢？

- 当对表中的数量进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。
- 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如过要简历聚簇索引，那么空间会更大。
- 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。

## 索引是如何提高查询速度的

将无序的数据变成相对有序的数据结构（就像查目录一样）（介绍B+树的结构）

## 使用索引的注意事项

- 在经常需要搜索的列上，可以加快搜索的速度；
- 在经常使用在**Where子句**中的列上创建索引，加快条件的判断速度；
- 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；
- 对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很多，不适合建索引（用一种技术来区分要查询哪一组数据,比如分区技术,水平分表技术）；
- 在经常用连接的列上，这些列上主要是一些外键，这些列主要是一些外键，可以加快连接的速度；
- 避免where子句中对字段施加函数，这样会造成无法命中索引；
- 在使用innoDB的时使用与业务无关的自增作为主键，即使用逻辑主键，而不要使用业务主键；
- 将打算加索引的键设置为NOT NULL，否则将导致引擎放弃使用索引而进行全表扫描；
- 删除长期未使用的索引，不用打索引的勋在会造成不必要的性能损耗，MySQL5.7可以通过查询sys库的chema_unused_indexes视图来查询那些索引从未被使用；
- 在使用limit offset查询缓慢时，可以借助索引来提高性能；

## MySQL索引主要使用的两种数据结构（还有全文索引）

哈希索引：对于哈希索引来说，底层数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最大；其余大部分场景，建议使用BTree索引。

BTree索引：MySQL的BTree索引使用的是B树中的B+树，但对于主要的两种存储引擎（MyISAM和innoDB）的实现方式是不同的。

🍎联合索引：联合索引(col1, col2,col3)也是一棵B+Tree，其非叶子节点存储的是第一个关键字的索引，而叶节点存储的则是三个关键字col1、col2、col3三个关键字的数据

![1570528527425](D:\-\学习笔记md\图片\1570528527425.png)

# 索引覆盖

如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为"覆盖索引"

我们知道再innoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要"回表”，也就是通过主键再查找一次，这样会比较慢，覆盖索引就是把查询出的列和索引是对应的，不做回表操作！

例子：现在我创建了索引（username，age），在查询select username, age from user where username='Java' and age=22.要查询的列在叶子节点都存在，所以就不用回表。

1.PRIMARYKEY（主键索引）

```
`mysql>altertabletable_nameaddprimarykey(`column`)`
```

2.UNIQUE或UNIQUEKEY(唯一索引)

```
`mysql>altertabletable_nameaddunique(`column`)`
```

3.FULLTEXT(全文索引)

```
`mysql>altertabletable_nameaddfulltext(`column`)`
```

4.INDEX(普通索引)

```
`mysql>altertabletable_nameaddindexindex_name(`column`)`
```

5.多列索引(聚簇索引)

```
`mysql>altertable`table_name`addindexindex_name(`column1`,`column2`,`column3`)`
```

修改表中的索引：

```
`altertabletablenamedropprimarykey,addprimarykey(fileda,filedb)`
```



## where子句和having子句的区别

一、where子句

​    where子句：where子句仅仅用于从from子句中返回的值，from子句返回的每一行数据都会用where子句中的条件进行判断筛选，where子句中允许使用比较运算符和逻辑运算符

二、having子句

​    having子句：having子句通常是与order by子句一起使用的，因为having的作用是对使用group by 进行分组统计后的结果进行进一步的筛选。

三、下面通过where子句和having子句的对比，更进一步的理解他们

​    在查询过程中聚合函数（SUM,MIN,MAX,AVG,COUNT）要比having子句优先执行，简单的理解为只有有了统计结果后我才能执行筛选。where子句在查询过程中执行优先级别优先于聚合函（SUM,MIN,MAX,AVG,COUNT），因为他是一句一句筛选的,**HAVING子句可以让我们筛选成组后的对各组数据筛选**。而WHERE子句在聚合前筛选记录，如：现在我们想要部门号不等于10的部门并且工资总和大于8000的部门编号？

​    分析：通过where子句筛选除部门编号不为10的部门，然后**对部门工资进行统计**，然后使用having子句对统计结果进行筛选。

```
 select deptno,sum(sa1) from emp
       where deptno!='10' 
	   group by deptno
       having sum(sa1)>8000;
```

四、异同点

​    他们的相同之处就是定义搜索条件，不同之处是where子句为单个筛选而having子句和组有关，而不是与单个的行有关

​     最后：理解having子句和where子句最好的方法就是基础select 语句中的那些句子的处理次序：where子句只能接受from子句输出的数据，而having子句则可以接受来自group by ，where或者from子句的输入。

having子句会在分组后对分组形成的结果进行过滤。

这个过程需要聚合、排序、因此如果通过where子句限制记录而省略掉having子句，是可以提升性能的。

https://www.cnblogs.com/leihaha/p/8202935.html

**顺序问题**：where子句必须放在group子句之前；而having子句必须在group子句之后

- **字段问题**：where子句只可以处理**数据表中的数据**；having只能处理在**group by子句中出现的字段**、**select的列的字段**或**聚合函数处理过的列**、**外部查询中的字段**。having根据前面查询出来的是什么就可以在后面接什么

　　1.having子句的字段必须出现在检索中（select的查询列、group by中的列、外查询中的列）或者是聚合函数处理后的列

2.where子句中的字段必须是数据表（物理表）中的列名，**不能是别名或者聚合函数**



sqlserver查询的执行顺序是：
(1)FROM <left_table> <join_type> JOIN <right_table> ON <on_predicate>
(2)WHERE <where_predicate>
(3)GROUP BY <group_by_specification>
(4)HAVING <having_predicate>
**(5)SELECT DISTINCT TOP(<top_specification>) <select_list>**
(6)ORDER BY <order_by_list>
所以在where执行的时候，别名还不存在，而order by的时候已经存在

where，having不能使用列别名（可以使用表别名）

```

where是在生成结果集前的操作，order by是生成结果集后的操作，因为where要生成结果集，而order by是对结果集的操作。
```

# B+树索引：

什么情况下应该建立b+树索引：cardinary值比较高（区分度比较高）的时候适合建立b+树索引

b+树结构：

![image-20190810151805600](D:\-\学习笔记md/图片/image-20190810151805600.png)

**根结点**和**中间结点**只存储**主键**，所有记录都在叶子结点（叶子结点存放地址）上，并且是顺序存放的。

可以有效减少IO次数，速度快。（假设一次IO可以从磁盘取500k的数据，如果主键和表项一起存，那么一次可以遍历的条目就会少很多。并且一次只能读一层）

​	聚集索引：对主键建立b+树，表项按照物理地址顺序存储。

​	非聚集索引（二次索引）：一张表只能有一个聚集索引，但可以有多个非聚集索引。定义：对于非主键建立的索引。先通过非聚集索引的b+树找到主键，然后用这个主键去聚集索引的b

+树找。

​	组合索引：最左前缀原则。

​	一棵树，有多个辅助索引集合在一起的。

例子：p6，money，age



**全文索引：**



**哈希索引**：自适应哈希索引，

## B+树和红黑树区别

一个用于硬盘一个用于内存

B+树适合用于硬盘，因为B+树的结构是扁平的，只在叶子结点存储数据，叶子结点是顺序存储的，IO次数少。

红黑树是近似自平衡的二叉树搜索树，检索速度快。

红黑树（Red-Black Tree）是二叉搜索树（Binary Search Tree）的一种改进。**是近似自平衡的。**我们知道二叉搜索树在最坏的情况下可能会变成一个链表（当所有节点按从小到大的顺序依次插入后）。而红黑树在每一次插入或删除节点之后都会花O（log N）的时间来对树的结构作修改，以保持树的平衡。也就是说，红黑树的查找方法与二叉搜索树完全一样；插入和删除节点的的方法前半部分节与二叉搜索树完全一样，**而后半部分添加了一些修改树的结构的操作。**

红黑树的每个节点上的属性除了有一个key、3个指针：parent、lchild、rchild以外，还多了一个属性：color。它只能是两种颜色：红或黑。而红黑树除了具有二叉搜索树的所有性质之外，还具有以下4点性质：

1. 根节点是黑色的。
2. 空节点是黑色的（红黑树中，根节点的parent以及所有叶节点lchild、rchild都不指向NULL，而是指向一个定义好的空节点）。
3. 红色节点的父、左子、右子节点都是黑色。
4. 在任何一棵子树中，每一条从根节点向下走到空节点的路径上包含的黑色节点数量都相同。




# 事务

（1） **事务的概念**：由一句或者一组SQL语句组成，是访问并更新数据库中各种数据项的一个程序执行单元，具有ACID特性。

（2） **ACID**：

​              原子性(atomicity):事务要么做，要么不做。

​              一致性（consistency）:事务执行前后数据库完整性约束不会被破坏。

​              隔离性（isolation）：事务在并发执行时的执行对象对其他事务分离。

​              持久性（durability）：事务一旦提交，则结果是永久性的。

（3） **事务的分类**：

扁平事务

带有保存点的扁平事务

链事务

嵌套事务

分布式事务

**（4）事务的实现**

（1）**redo log(重做日志)**:

功能:保证持久性，在服务器重启时检查重做日志保证事务的持久性。

log内容:物理页信息(性能优于binlog)。

组织结构:重做日志缓冲+重做日志文件。

参数innodb_flush_log_at_trx_commit用来控制重做日志刷新到磁盘的策略：

选择级别:0(每隔1s master thread进行写入，执行fsync操作)；2(每次提交事务写入缓冲，快但是有风险)

存储结构:log block

log生命周期：持久化

（2）**undo log (事务提交后purge)**:

功能:保存事务的回滚信息。

log内容:物理页信息。

存储结构:rollback段(限制最大并发事务量)* undolog页(多个事务可重用)

purge操作:根据history list进行删除。

Log生命周期： 事务结束后purge thread根据时机清除。

## 事务隔离级别

 SQL 标准定义了四个隔离级别: 

**READ-UNCOMMITTED(读取未提交)**: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻 读或不可重复读
 **READ-COMMITTED(读取已提交):** 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读 仍有可能发生 

**REPEATABLE-READ(可重复读)**: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修 改，可以阻止脏读和不可重复读，但幻读仍有可能发生。
 **SERIALIZABLE(可串行化)**: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务 之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 、

事务隔离级别和对应的封锁级别

1.读未提交(read uncommit)

一级封锁协议(读取数据的时候不加锁，更新的时候整个加X锁)

b事物执行到一半，a事物不检测锁直接读取，结果b事物回滚了，导致a事物读出了一个错的结果，这就是脏读。

2.读已提交(read committed)

二级封锁协议(读取数据的时候加S锁，更新的时候加X锁)

a事物每次读取的时候都会尝试获取s锁，如果b事物在更新，则a阻塞等待b事物释放。

解决了脏读的问题。

因为a读取完毕以后直接释放，如果a事物有两次读取操作，而在这期间b更新了数据，

会导致两次结果不一样。这就是不可重复读

3.可重复读(repeatable read)

三级封锁协议(对读取数据的整个事务加S锁，更新的时候整个加X锁)

因为a读取操作整个加了s锁，所以在此期间b事物不能获得x锁。这样就解决了不可重复读问题。

幻读a在读表的时候，b往表里删除行，导致a统计的结果和表里现在的不一致，好像发生了幻觉。(幻读来源于表行的增加或删除，解决幻读只能对表加锁)

4.序列化(sirializable)

锁全表(事物不并发执行)

解决了幻读现象。

# 日志

**redo log(重做日志)**:

存放在：重做日志文件

功能:保证持久性，在服务器重启时检查重做日志保证事务的持久性。

log内容:物理页信息(性能优于binlog)。

组织结构:重做日志缓冲+重做日志文件。

选择级别:0(每隔1s master thread进行写入，执行fsync操作)2(每次提交事务写入缓冲，快但是有风险)

存储结构:log block

log生命周期：持久化

**undo log (事务提交后purge)**:

存放在：undo存放在数据库内部一个特殊段中（segment），这个段称为undo段，位于共享表内存空间中。

功能:保存事务的回滚信息，利用undo信息将数据回滚到修改之前的样子。**另一个作用是MVCC，即在InnoDB存储引擎中MVCC的实现是通过undo来完成。当用户读取一行记录时，若该记录已经被其它事务占用，当前事务可以通过undo读取之前行的版本信息，以此实现非锁定读取。**

log内容:物理页信息。

存储结构:rollback段(限制最大并发事务量)* undolog页(多个事务可重用，事务提交时，首先将undo log放入链表中，然后判断undo页的使用空间是否小于**3/4**，若是则表示undo页可以被重用)

purge操作:**根据history list进行删除，事务提交后并不能马上删除undo log及undo log所在的页，这是因为可能还有其它事务需要通过undo log来得到行记录之前的版本。故事务提交时将undo log 放入一个链中，是否可以最终删除undo log及undo log所在的页由purge线程判断。**

Log生命周期： 事务结束后purge thread根据时机清除。

undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中，但从5.6开始，也可以使用独立的Undo 表空间。

Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作（例如[bug#69812](http://bugs.mysql.com/bug.php?id=69812)）。

大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除（谁会对刚插入的数据有可见性需求呢！！），而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo。

binlog：全量数据迁移或者分布式数据库增量同步时使用



# 并发控制

锁和mvcc（mvcc通过undo log）



幻读：当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行。

对实时性要求不高对情况：通过**多版本并发控制**解决了幻读的问题（并发性高，可能读取到历史数据）

innodb用通过Next-Key Lock锁定事务需要读取的行，解决幻读问题。

脏读是读取到未提交的数据（需要将事务隔离级别设置为read uncommitted才会产生）

不可重复读读取到的是已经提交的数据。

# 锁

<https://juejin.im/post/5aaf6ee76fb9a028d3753534>

锁是数据库系统区别于文件系统的一个关键特性

数据库系统使用锁是为了**支持对共享资源进行并发访问**，提供数据完整性和一致性。

在数据库在中，lock与latch都可以称为锁。

## lock与latch

latch一般被成为轻量级锁，因为其要求锁定的时间必须非常短。若持续时间长，则应用的性能会非常差。在InnoDB引擎中，latch又可以分为mutex（互斥量）和rwlock（读写锁）。其目的是用来保证并发线程操作临界资源的正确性，并且通常没有死锁检测机制。

lock的对象是事务，是用来锁定数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit或rollback后进行释放（不同事务隔离级别释放的时间可能不同）。有死锁机制。

|          | lock                                                  | latch                                                        |
| -------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| 对象     | 事务                                                  | 线程                                                         |
| 保护     | 数据库内容                                            | 内存数据结构                                                 |
| 持续时间 | 整个事务过程                                          | 临界资源                                                     |
| 模式     | 行锁、表锁、意向锁                                    | 读写锁、互斥量                                               |
| 死锁     | 通过waits-for graph、time out等机制进行死锁检测与处理 | 无死锁检测与处理机制。仅通过应用程序加锁的顺序（lock leveling）保证无死锁的情况发生 |
| 存在于   | lock Manager的哈希表中                                | 每个数据结构随想中                                           |

基本的行级锁有两种：**排他锁**和**共享锁**

 (1) 共享锁(S锁)：又称读锁，允许事务读取一行数据。

​       (2) 排它锁(X锁)：又称写锁，允许事务删除或更新一行数据。

​       (3) 意向共享锁(IS锁)：事务想要获得一张表中某几行数据共享锁。

​       (4) 意向排它锁(IX锁)：事务想要获得一张表中某几行数据的排它锁。

   

##   锁的兼容性

​       一致性非锁定读：指InnoDB通过多版本控制的方式读取快照数据（历史版本的数据），不进行加锁，提高数据库的读取效率。这是默认的读取方式。

​       一致性锁定读：指InnoDB为了保证数据逻辑的一致性进行加锁读取。可以通过”SELECT ..FOR UPDATE””SELECT …LOCK IN SHARE MODE”进行显式加锁。

------------------------------------------------------------------------------

​       自增长与锁：对于Mysql的自增长表，，会使用**AUTO-INC Locking**（一种特殊的表锁机制，为了提高插入的性能，锁不是在一个事务完成后才释放，二十完成对自增长值插入的sql语句后立即释放），锁定一列，不必等待事务完成。

ps：从mysql5.1.22版本开始，InnoDB引擎中提供了一种**轻量级互斥量**的**自增长实现机制**，这种机制大大提高了自增长值插入的性能。（可以通过参数innodb_lock_mode来控制自增长模式，默认是1，0表示使用原来的**AUTO-INC Locking**方式；1表示对于simple_inserts（指能在插入前就确定插入行数的语句，insert，replace等），该模式会使用互斥量去对内存中的计数器进行累加的操作，对于bulk inserts还是会使用传统表锁的AUTO-INC locking方式。如果已经使用AUTO-INC Locking方式去产生自增长的值，而这是需要再进行simple inserts的操作时，这是需要等待AUTO-INC Locking的释放。2.全部使用互斥量。）

​       外键与锁：对外键的插入与更新，会先访问父表，对父表进行一致性锁定读的加锁。

#### 活锁

活锁是某事务可能永远等待，这是活锁的情形

解决方法：简单方法是采用先来先服务的策略。当多个事务请求封锁同意数据时，封锁子系统请求锁的先后次序对事务进行排队，数据对象上的锁一旦释放就批准申请队列中第一个事务获得锁。

#### 死锁

由于两个或多个事务都已经封锁了一些数据对象，然后又请求已经被事务锁住的数据，这些事务永远都不能结束，就形成了死锁。

预防死锁：

**一次封锁法**：要求每个事务必须一次将所有要使用的数据全部加锁，否则不能执行下去。一次封锁法虽然可以有效的防止死锁的发生，但是增加了锁的粒度，从而降低了系统的并发性。并且数据库时不断变化的，所以事先很难精确地确定每个事务所需进行加锁的对象，为此只能扩大封锁范围，将事务执行可能需要封锁的数据对象全部加锁，这就进一步降低了并发度。

**顺序封锁法**：预先对数据对象规定一个封锁顺序，所有事务都按这个顺序实施封锁。（很难按规定的顺序去实施加锁，因为事先难以确定每一个事务要封锁哪些对象）

由此可见**数据库不适合预防死锁**

#### 死锁的诊断与解除

**诊断：**

1.超时法：如果一个事务的等待时间超过了时限，那么就认为其发生了死锁。

优点：实现简单

不足：1。可能误判死锁：如事务因为其他原因而使等待时间超过时限，系统就会误认为发生了死锁；若时限设置得太长，则不能及时发现死锁。

2.事务等待图法：事务等待图是一个有向图G=（T，U），T为结点的集合，每个节点表示正在运行的事务。事务等待图动态地反应了所有事务的等待情况，并发控制子系统周期性（比如每隔数秒）生成事务等待图，并进行检测，如果发现图中存在回路，则表示系统中出现了死锁。

检测工具：

Jstack工具可以用于生成java虚拟机当前时刻的线程快照。线程快找是当前java虚拟机内每一条线程正在执行的方法对战的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如`线程间死锁`、`死循环`、`请求外部资源导致的长时间等待`等。

`jps`确定当前执行任务的进程号

`jstack -F 1362`

**解除：**

选择一个处理死锁代价最小的事务，将其撤销，释放此事务持有的所有锁。（对撤销的事务锁进行的数据修改加以恢复）

## 两段锁协议

两段锁协议规定所有的事务应遵守的规则：

① 在对任何数据进行读、写操作之前，首先要申请并获得对该数据的封锁。

② 在释放一个封锁之后，事务不再申请和获得其它任何封锁。

即事务的执行分为两个阶段：

第一阶段是获得封锁的阶段，称为扩展阶段。

第二阶段是释放封锁的阶段，称为收缩阶段。

若所有事务均遵守两段锁协议，则这些事务的所有交叉调度都是可串行化的。





### 2.3 InnoDB关键特性：为InnoDB提供更好的性能以及可靠性

​     (1) 插入缓冲（Insert Buffer）：解决离散写问题    

​    (2）两次写（Double Write）：解决数据可靠性（宕机页失效）

​    (3) 自适应哈希索引(Adaptive Hash Index)：提高读写效率

​    (4) 异步IO(Async IO)：加速io，合并io

​    (5) 刷新邻接页 (Flush Neighbor Page)：合并io

# 引擎

innoDB与MyISAM引擎的区别

两者都适用B+树作为索引结构

不同点

| innoDB                                   | MyISAM                                           |
| ---------------------------------------- | ------------------------------------------------ |
| 叶子节点保存了完整的数据记录（聚集索引） | 叶子结点记录的是地址（非聚集索引）               |
| 事务安全型                               | 非事务安全型                                     |
| 支持行级锁定                             | 锁的粒度是表级                                   |
| 不支持全文索引（5.6后支持）              | 支持全文索引                                     |
|                                          | 保存成文件的形式，在咖平台转移中使用会省去不少事 |

应用场景：

MyISAM管理非事务表，它提供高速存储和检索，以及全文搜索能里，如果应用中需要执行大量的select查询，那么MyISAM是更好的选择；

InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持，如果应用中需要执行大量的INSERT或UPDATE，则应该使用innoDB，这样可以提高**用户并发操作**的性能。

innoDB中辅助索引：辅助索引data域存放的是相应记录的主键而不是地址，先取出主键的值，再走一遍主索引。（因此在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会导致主索引频繁分裂）